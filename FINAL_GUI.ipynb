{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958bee62",
   "metadata": {},
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "encountered an unsupported criticial chunk type \"orNT\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-70879697fadc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mmainwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1200x1200'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mmainwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresizable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhotoImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"E:\\projects\\AUTOMATIC\\Automatic Interview\\wp10281364.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\face\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, cnf, master, **kw)\u001b[0m\n\u001b[0;32m   3543\u001b[0m         \u001b[0mValid\u001b[0m \u001b[0mresource\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3544\u001b[0m         width.\"\"\"\n\u001b[1;32m-> 3545\u001b[1;33m         \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'photo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3546\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mblank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3547\u001b[0m         \u001b[1;34m\"\"\"Display a transparent image.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\face\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, imgtype, name, cnf, master, **kw)\u001b[0m\n\u001b[0;32m   3499\u001b[0m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3500\u001b[0m             \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3501\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'create'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3502\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3503\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: encountered an unsupported criticial chunk type \"orNT\""
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter.messagebox import showinfo\n",
    "from gtts import gTTS\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import json\n",
    "import random\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import pyttsx3\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "mainwindow= Tk()\n",
    "mainwindow.title('Face recognition')\n",
    "mainwindow.geometry('1200x1200')\n",
    "mainwindow.resizable(0, 0)\n",
    "p = PhotoImage(file=r\"E:\\projects\\AUTOMATIC\\Automatic Interview\\wp10281364.png\")\n",
    "l = Label(image=p)\n",
    "l.place(x=0,y=0)\n",
    "path = r\"E:\\projects\\AUTOMATIC\\Automatic Interview\\face-trainner.yml\"\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def face():\n",
    "    global na\n",
    "    na = \"unknown\"\n",
    "    labels = [\"pavan\", \"Soud\",\"Waseem\"] \n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier(r'C:\\Users\\sujit\\Downloads\\haarcascade_frontalface_default.xml')\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    recognizer.read(path)\n",
    "\n",
    "    cap = cv2.VideoCapture(0) #Get vidoe feed from the Camera\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        ret, img = cap.read() # Break video into frames \n",
    "        gray  = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #convert Video frame to Greyscale\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5) #Recog. faces\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w] #Convert Face to greyscale \n",
    "\n",
    "            id_, conf = recognizer.predict(roi_gray) #recognize the Face\n",
    "\n",
    "            if conf>=80:\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX #Font style for the name \n",
    "                name = labels[id_] #Get the name from the List using ID number \n",
    "                na = name\n",
    "                cv2.putText(img, name, (x,y), font, 1, (0,0,255), 2)\n",
    "\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "        cv2.imshow('Preview',img) #Display the Video\n",
    "        if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "            break\n",
    "     # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #print(na)\n",
    "    if na in labels:\n",
    "        nex = Toplevel(mainwindow)\n",
    "        nex.title('Embedded programs')\n",
    "        nex.geometry(\"600x600\")\n",
    "        p = PhotoImage(file=r\"C:\\Users\\sujit\\Downloads\\beautiful-wavy-background_23-2148465605.png\")\n",
    "        l = Label(nex,image=p)\n",
    "        l.place(x=0,y=0)\n",
    "        \n",
    "        Label(nex, text=\"welcome \" + na, font=(\"Times New Roman\", 25), bg='white').place(x=200, y=50)\n",
    "        texttospeechbutton = Button(nex, text='Text-To-Speech Conversion', font=('Times New Roman', 16), bg='Cyan', command=TextToSpeech)\n",
    "        texttospeechbutton.place(x=50, y=250)\n",
    "\n",
    "        speechtotextbutton = Button(nex, text='Speech-To-Text Conversion', font=('Times New Roman', 16), bg='Cyan', command=SpeechToText)\n",
    "        speechtotextbutton.place(x=50, y=400)\n",
    "\n",
    "        c = Button(nex,text=\"eye ball tracking\",font=('Times New Roman', 16), bg='Cyan',command=eye).place(x=400, y=250)\n",
    "        d = Button(nex,text=\"emotion detector\",font=('Times New Roman', 16), bg='Cyan',command=emotion).place(x=400, y=400) \n",
    "        f = Button(nex,text=\"Quit\",font=('Times New Roman', 15), bg='Red',command=nex.destroy).place(x=500, y=500)\n",
    "        nex.mainloop()\n",
    "        \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def eye():\n",
    "    eye_cascade = cv2.CascadeClassifier('eye.xml')\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while 1:\n",
    "        ret, img = cap.read()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        eyes = eye_cascade.detectMultiScale(gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "            roi_gray2 = gray[ey:ey+eh, ex:ex+ew]\n",
    "            roi_color2 = img[ey:ey+eh, ex:ex+ew]\n",
    "            circles = cv2.HoughCircles(roi_gray2,cv2.HOUGH_GRADIENT,1,20,param1=50,param2=30,minRadius=0,maxRadius=0)\n",
    "            try:\n",
    "                for i in circles[0,:]:\n",
    "                    # draw the outer circle\n",
    "                    #cv2.circle(roi_color2,(int(i[0]),int(i[1])),int(i[2]),(255,255,255),2)\n",
    "                    cv2.circle(roi_color2,(i[0],i[1]),i[2],(255,255,255),2)\n",
    "                    print(\"drawing circle\")\n",
    "                    # draw the center of the circle\n",
    "                    #cv2.circle(roi_color2,(int(i[0]),int(i[1])),2,(255,255,255),3)\n",
    "                    cv2.circle(roi_color2,(i[0],i[1]),2,(255,255,255),3)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        cv2.imshow('img',img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def emotion():\n",
    "    face_classifier = cv2.CascadeClassifier(r'C:\\Users\\sujit\\Downloads\\haarcascade_frontalface_default.xml')\n",
    "    classifier =load_model('Emotion_little_vgg.h5')\n",
    "\n",
    "    class_labels = ['Angry','Happy','Neutral','Sad','Surprise']\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # Grab a single frame of video\n",
    "        ret, frame = cap.read()\n",
    "        labels = []\n",
    "        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            roi_gray = gray[y:y+h,x:x+w]\n",
    "            roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "        # rect,face,image = face_detector(frame)\n",
    "\n",
    "\n",
    "            if np.sum([roi_gray])!=0:\n",
    "                roi = roi_gray.astype('float')/255.0\n",
    "                roi = img_to_array(roi)\n",
    "                roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "            # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "                preds = classifier.predict(roi)[0]\n",
    "                label=class_labels[preds.argmax()]\n",
    "                label_position = (x,y)\n",
    "                cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "            else:\n",
    "                cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        cv2.imshow('Emotion Detector',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def say(text1):\n",
    "        language = 'en'\n",
    "        speech = gTTS(text = text1, lang = language, slow = False)\n",
    "        speech.save(\"text.mp3\")\n",
    "        os.system(\"start text.mp3\")\n",
    "\n",
    "def recordvoice():\n",
    "    while True:\n",
    "        r = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            audio=r.listen(source)\n",
    "            try:    \n",
    "                text1 = r.recognize_google(audio,language=\"en-IN\")\n",
    "            except:\n",
    "                pass\n",
    "            return text1\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def TextToSpeech():\n",
    "    texttospeechwindow = Toplevel(mainwindow)\n",
    "    texttospeechwindow.title('Text-to-Speech')\n",
    "    texttospeechwindow.geometry(\"500x500\")\n",
    "    texttospeechwindow.configure(bg='Blue')\n",
    " \n",
    "    Label(texttospeechwindow, text='Text-to-Speech Converter', font=(\"Times New Roman\", 15), bg='Blue').place(x=50)\n",
    " \n",
    "    text = Text(texttospeechwindow, height=5, width=30, font=12)\n",
    "    text.place(x=7, y=60)\n",
    "   \n",
    "    speakbutton = Button(texttospeechwindow, text='listen', bg='coral', command=lambda: say(str(text.get(1.0, END))))\n",
    "    speakbutton.place(x=140, y=200)\n",
    "\n",
    "def SpeechToText():\n",
    "    speechtotextwindow = Toplevel(mainwindow)\n",
    "    speechtotextwindow.title('Speech-to-Text Converter by')\n",
    "    speechtotextwindow.geometry(\"500x500\")\n",
    "    speechtotextwindow.configure(bg='pink')\n",
    "    Label(speechtotextwindow, text='Speech-to-Text', font=(\"Comic Sans MS\", 15), bg='IndianRed').place(x=50)\n",
    " \n",
    "    text = Text(speechtotextwindow, font=12, height=3, width=30)\n",
    "    text.place(x=7, y=100)\n",
    "   \n",
    "    recordbutton = Button(speechtotextwindow, text='Record', bg='Sienna', command=lambda: text.insert(END, recordvoice()))\n",
    "    recordbutton.place(x=140, y=50)\n",
    "\n",
    "Label(mainwindow, text='Wanna recognize your face!!',\n",
    "     font=('Times New Roman', 25), bg='white', wrap=True, wraplength=450).place(x=400, y=50)\n",
    "\n",
    "e = Button(mainwindow,text=\"face recognition \",font=('Times New Roman', 20), bg='White',command=face).place(x=500, y=350)\n",
    "g = Button(mainwindow,text=\"Quit\",font=('Times New Roman', 15), bg='Red',command=mainwindow.destroy).place(x=1100, y=700)\n",
    "\n",
    "mainwindow.update()\n",
    "mainwindow.mainloop()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d63eda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf2fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
